{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba 1: An치lisis de Sentimientos de Twitter\n",
    "\n",
    "<br>\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Nombre del Grupo:</b> Tusca.\n",
    "\n",
    "<b>Integrante 1:</b> Juan Jose Uribe Mella.\n",
    "\n",
    "<b>Integrante 2:</b> Nicol치s Ignacio G칩mez Espejo.\n",
    "\n",
    "<b>Integrante 3:</b> Rafael Ignacio Mascayano O'Ryan.\n",
    "\n",
    "<b>Integrante 4:</b> Javier Ignacio L칩pez Sanhueza.\n",
    "\n",
    "               \n",
    "\n",
    "<b>Generaci칩n:</b> Generaci칩n 2.\n",
    "\n",
    "<b>Profesor:</b> Gabriel Tamayo L.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A  continuaci칩n  se  presenta un  problema cl치sico en  el  an치lisis de  texto: Extraer el  sentimiento asociado a un texto. Para esto, utilizaremos una base de datos provenientes de CrowdFlower. Para descargar los datos puede ejecutar el siguiente c칩digo: \n",
    "\n",
    "`wget https://www.crowdflower.com/wp-content/uploads/2016/07/text_emotion.csv`\n",
    "\n",
    "El objetivo general de esta prueba es alcanzar el mejor desempe침o posible para clasificar si un tweet es positivo o negativo. Para medir el desempe침o, se evaluar치 con un conjunto de datos del cu치l no tendr치n acceso. De esta manera evitaremos que los modelos aprendan informaci칩n sobre el conjunto de validaci칩n."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Requerimientos\n",
    "\n",
    "Realizaremos un an치lisis de sentimientos de un Tweet para saber si este est치 asociado a un sentimiento positivo o a un sentimiento negativo.\n",
    "\n",
    "![Example](tweet-example.png)\n",
    "\n",
    "Para esto utilizaremos la base de datos provenientes de _CrowdFlower_.\n",
    "\n",
    "## Vector Objetivo\n",
    "\n",
    "El vector objetivo ser치 una variable binaria que indicar치 si el sentimiento es positivo o no.\n",
    "\n",
    "## M칠tricas a utilizar\n",
    "\n",
    "Como este es un problema de clasificaci칩n, nos centraremos en la predicci칩n de la clasificaci칩n para una nueva observaci칩n. Para lo cual se utilizar치n las siguientes m칠tricas:\n",
    "\n",
    "- **Matrix de confusi칩n**: cantidad de observaciones predichas de forma correcta.\n",
    "- **Accuracy** (exactitud): porcentaje de casos predichos correctamente por sobre el total de casos.\n",
    "- **Precision**: mide la fracci칩n de predicciones correctas entre las etiquetas positivas.\n",
    "- **Recall**: Mide la fraccion de verdaderos positivos predichos por el modelo.\n",
    "- **F1**: representa la media arm칩nica entre Precision y Recall (es decir, una medida general de la presici칩n).\n",
    "- **ROC** (en particular, _AUC_): eval칰a la relaci칩n entre ambos errores (falsos positivos y falso negativo) condicional en todo el rango del clasificador.\n",
    "\n",
    "De la misma forma se usar치n modelos de clasificaci칩n entre los que destaca:\n",
    "- Regresi칩n Log칤stica,\n",
    "- M치quina de Soporte Vectorial,\n",
    "- Naive Bernoulli,\n",
    "- Random Forest,\n",
    "- Ridge,\n",
    "- AdaBoost,\n",
    "- Redes Neuronales,\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An치lisis Exploratorio\n",
    "\n",
    "Se procede a evaluar las variables y explorar de forma grafica el comportamientos de aquellas. Previo a aquello se procede a cargar las librerias necesarias y b치sicas de un data scientist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'helpers'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-68fa14fa2983>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;31m# Import helpers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhelpers\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnicos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_style\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"darkgrid\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'helpers'"
     ]
    }
   ],
   "source": [
    "# Importar librer칤as b치sicas\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Importaci칩n de librer칤as cl치sicas\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import missingno as msgo\n",
    "\n",
    "# Importaci칩n de librer칤as de ML\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.linear_model import LogisticRegression, RidgeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, roc_auc_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Importaci칩n de librer칤as para el tratamiento de texto\n",
    "import nltk\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Importaci칩n de librer칤a para la persistencia del modelo  \n",
    "import pickle\n",
    "\n",
    "# Import helpers\n",
    "import helpers as nicos\n",
    "\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se carga el la base de datos entregada en la web Desafio Latam. \n",
    "# Se observa que es necesario eliminar la columna con \"Unnamed: 0\", debido a que no entrega informaci칩n\n",
    "\n",
    "df = pd.read_csv('training_tweets.csv').drop('Unnamed: 0', axis=1)\n",
    "\n",
    "# Se evalua el contenido y el tipo de dato en su interior. De forma paralela se busca la existencia de NaN.\n",
    "df.info()\n",
    "\n",
    "msgo.matrix(df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que el dataset no contiene valores NaN. \n",
    "\n",
    "### Vector Objetivo\n",
    "\n",
    "Veamos como distribuye el vector objetivo sin preprocesar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20, 10))\n",
    "sns.countplot(df.sentiment, order=df.sentiment.value_counts().index);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notar que hay muchos datos neutrales. Para no afectar el resultado, en la etapa de _feature engineering_ vamos a eliminar los registros neutrales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos\n",
    "\n",
    "El 칰nico atributo es `content` el cual es el texto con el Tweet. A continuaci칩n veremos cuales son las palabras m치s repetidas considerando todas las observaciones no neutrales:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicos.most_common_words(df[df.sentiment != 'neutral'].content.str.lower())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras asociadas a sentimientos positivos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicos.most_common_words(df[df.sentiment.isin(['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm'])].content.str.lower())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las palabras asociadas a sentimientos negativos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nicos.most_common_words(df[~df.sentiment.isin(['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm'])].content.str.lower())[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento y Feature Engineering\n",
    "\n",
    "### Estrategia de Preprocesamiento\n",
    "\n",
    "Se propone realizar un preprocesamiento de los datos, por medio de lo sugerido en el enunciado con Lemantizaci칩n. Para esto se debe estudiar las funciones de la librer칤a NTLK. De la misma forma, se puede proceder a separar las palabras de los comentarios con CountVectorizer o con TfidVectorizer. \n",
    "Luego de este preprocesamiento, se aplicaran los modelos, evaluando cual de ellos es el mejor, de las misma forma se planea utilziar un modelo de votaci칩n de ensamble para evaluar los mejores. De esta forma, se utilizar치n los modelos con ambos preprocesamientos, y se eligir치 el mejor modelo en tiempo de procesamiento y resultados.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primero filtramos sentimientos que no son neutrales\n",
    "df = df[df.sentiment != 'neutral']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Objetivo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ahora generamos el vector objetivo binarizado\n",
    "positive_elements = ['happiness', 'love', 'surprise', 'fun', 'relief', 'enthusiasm']\n",
    "y = np.where(np.isin(df.sentiment, positive_elements), 1, 0)\n",
    "\n",
    "sns.countplot(y);\n",
    "plt.title('Distribuci칩n de Vector Objetivo (positivo = 1)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ambas clases est치n balanceadas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atributos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicaremos lemantizaci칩n y tokenizaci칩n sobre los Tweets.\n",
    "\n",
    "Guardamos en `X` la matriz de atributos (para este ejercicio, es s칩lo 1 columna):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el preprocesamiento, utilizaremos las stopwords del diccionario ingl칠s junto a otros signos que no aportan informaci칩n en la clasificaci칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words(\"english\") + ['!','.','@',',','...','?','s', '&','/',';','-','..','\"', '췋']\n",
    "stopwords = set(stopwords)\n",
    "tokenizer = nicos.LemmaTokenizer()\n",
    "tokenized_stopwords = tokenizer(' '.join(stopwords))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crearemos un objeto con `CountVectorizer` para utilizarlo en los modelos:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(tokenizer=nicos.LemmaTokenizer(),\n",
    "                                   strip_accents = 'unicode',\n",
    "                                   stop_words = tokenized_stopwords,\n",
    "                                   max_features= 2000,\n",
    "                                   lowercase = True)\n",
    "\n",
    "count_model_fit = count_vectorizer.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las siguientes son las 10 palabras m치s repetidas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_common = pd.DataFrame(count_model_fit.toarray(), columns = count_vectorizer.get_feature_names())\n",
    "most_common = most_common.sum().sort_values(ascending=False)\n",
    "most_common[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementaci칩n de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test, = train_test_split(X, y, test_size = 0.33, random_state = 4092019)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Primer Modelo: NaiveBernoulli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pip_bernoulli = Pipeline([('countV', count_vectorizer),('NB',BernoulliNB())])\n",
    "model_pip_bernoulli.fit(x_train, y_train)\n",
    "print(classification_report(y_test,model_pip_bernoulli.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Segundo Modelo: Regresi칩n Log칤stica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pip_LR = Pipeline([('countV', count_vectorizer),('Lg', LogisticRegression(random_state=4092019))])\n",
    "model_pip_LR.fit(x_train, y_train)\n",
    "print(classification_report(y_test,model_pip_LR.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "tfid = TfidfVectorizer(tokenizer=nltk.tokenize.TweetTokenizer().tokenize,\n",
    "                                   strip_accents = 'unicode',\n",
    "                                   stop_words = nltk.tokenize.TweetTokenizer().tokenize(' '.join(stopwords)),\n",
    "                                   lowercase = True)\n",
    "\n",
    "model_pip_LR = Pipeline([('countV', tfid),('Lg', LogisticRegression(random_state=4092019))])\n",
    "model_pip_LR.fit(x_train, y_train)\n",
    "\n",
    "print(classification_report(y_test,model_pip_LR.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tercer Modelo: M치quina de Soporte Vectorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "gamma = [0.0000001, 0.0001, 0.001, 0.01, 0.1, 1, 10]\n",
    "\n",
    "parameters_first = {\n",
    "    \"kernel\":[\"rbf\"],\n",
    "    \"C\": C[:3],\n",
    "    \"gamma\": gamma[:3]\n",
    "}\n",
    "\n",
    "parameters_second = {\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"C\": C[3:],\n",
    "    \"gamma\": gamma[3:],\n",
    "}\n",
    "\n",
    "parameters_third = {\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"C\": C[:3],\n",
    "    \"gamma\": gamma[3:]\n",
    "}\n",
    "\n",
    "parameters_fourth = {\n",
    "    \"kernel\": [\"rbf\"],\n",
    "    \"C\": C[3:],\n",
    "    \"gamma\": gamma[:3]\n",
    "}\n",
    "\n",
    "def get_gs_svm(params):\n",
    "    return Pipeline([\n",
    "        ('countV', count_vectorizer),\n",
    "        ('grid', GridSearchCV(SVC(random_state=4092019, probability=True), param_grid=params, cv=2, n_jobs=-1, refit=True)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dado el tiempo de ejecuci칩n se separa en dos modelos\n",
    "model_GS_SVM_first = get_gs_svm(parameters_first)\n",
    "model_GS_SVM_first.fit(x_train, y_train)\n",
    "print(classification_report(y_test, model_GS_SVM_first.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Segundo modelo de SVM con segundo grupo de hiperpar치metros\n",
    "model_GS_SVM_second = get_gs_svm(parameters_second)\n",
    "model_GS_SVM_second.fit(x_train, y_train)\n",
    "print(classification_report(y_test, model_GS_SVM_second.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tercer modelo de SVM con tercer grupo de hiperpar치metros\n",
    "model_SVC_third = get_gs_svm(parameters_third)\n",
    "model_SVC_third.fit(x_train, y_train)\n",
    "print(classification_report(y_test, model_SVC_third.predict(x_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cuarto modelo de SVM con tercer grupo de hiperpar치metros\n",
    "model_SVC_fourth = get_gs_svm(parameters_fourth)\n",
    "model_SVC_fourth.fit(x_train, y_train)\n",
    "print(classification_report(y_test,model_SVC_fourth.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cuarto Modelo: Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"n_estimators\": list(np.linspace(20, 1000, 50, dtype = \"int\")),\n",
    "    \"max_features\": [None, \"log2\", \"sqrt\"]\n",
    "}\n",
    "\n",
    "rfc = RandomForestClassifier(\n",
    "    oob_score = True,\n",
    "    criterion = \"gini\",\n",
    "    max_depth = 16,\n",
    "    random_state = 4092019\n",
    ")\n",
    "\n",
    "model_GS_rf = Pipeline([('countV', count_vectorizer),\n",
    "                         ('grid', GridSearchCV(rfc, param_grid = parameters, cv = 2, n_jobs=-1))])\n",
    "\n",
    "model_GS_rf.fit(x_train, y_train)\n",
    "\n",
    "print(classification_report(y_test, model_GS_rf.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quinto Modelo: Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'alpha': [1e-3, 1e-2, 1e-1, 1],\n",
    "    'normalize': [True, False],\n",
    "}\n",
    "\n",
    "model_GS_ridge = Pipeline([\n",
    "    ('countV', count_vectorizer),\n",
    "    ('grid', GridSearchCV(RidgeClassifier(random_state=4092019), param_grid=parameters, cv = 2, n_jobs=-1))\n",
    "])\n",
    "\n",
    "model_GS_ridge.fit(x_train, y_train)\n",
    "print(classification_report(y_test, model_GS_ridge.predict(x_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Elecci칩n de modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para tener m치s informaci칩n sobre cual modelo elegir, calcularemos el AUC score y adem치s las curvas ROC de cada modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_curves = {}\n",
    "\n",
    "models = [\n",
    "    ('bernoulli', model_pip_bernoulli),\n",
    "    ('LR', model_pip_LR),\n",
    "    ('SVM', model_GS_SVM_second),\n",
    "    ('RF', model_GS_rf),\n",
    "    ('Ridge', model_GS_ridge),\n",
    "]\n",
    "\n",
    "for i, (name, model) in enumerate(models):\n",
    "    if name == 'Ridge': # https://stackoverflow.com/a/22587041\n",
    "        d = model.decision_function(x_test)\n",
    "        prob = np.exp(d) / np.sum(np.exp(d))\n",
    "    else:\n",
    "        prob = model.predict_proba(x_test)[:, 1]\n",
    "    roc_curves[i] = roc_curve(y_test, prob)\n",
    "    print(name, 'AUC:', roc_auc_score(y_test, prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "plt.figure(figsize=(20, 10))\n",
    "\n",
    "plt.title('Curva ROC')\n",
    "\n",
    "\n",
    "for i, (name, model) in enumerate(models):\n",
    "    plt.plot(roc_curves[i][0], roc_curves[i][1], lw=1, label=name)\n",
    "\n",
    "plt.plot([0, 1], ls=\"--\", lw=1)\n",
    "plt.ylabel('Verdaderos Positivos')\n",
    "plt.legend(prop={'size': 15})\n",
    "plt.xlabel('Falsos Positivos');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los puntajes AUC son parecidos, siendo el mayor el modelo de regresi칩n.\n",
    "\n",
    "Los modelos m치s estables en base a las m칠tricas de `presicion`, `recall` y `f1-score` son LR y Ridge (estos tienen todas las m칠tricas sobre 0.7). Debido a esto sumado a que LR tiene mejor AUC, nos quedaremos con el modelo de regresi칩n:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model_pip_LR, open(\"grupo_nicos_&_tuskas_model_pip_LR.sav\",\"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PD:\n",
    "\n",
    "쯏 qu칠 ocurre con el Tweet de la imagen al comienzo?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet = \"Democrat Congresswoman totally fabricated what I said to the wife of a soldier who died in action (and I have proof). Sad!\"\n",
    "\n",
    "for i, (name, model) in enumerate(models):\n",
    "    prediction = model.predict([tweet])[0]\n",
    "    if prediction == 1:\n",
    "        print(name, ':', '游녨')\n",
    "    else:\n",
    "        print(name, ':', '游땨')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tal y como era de esperar 游땒"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
